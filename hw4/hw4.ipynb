{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 4: Generating and Finetuning Transformer Language Models With Huggingface ","metadata":{"id":"CeUOYktpXtOz"}},{"cell_type":"markdown","source":"In this project, you will first learn how to use Huggingface's Transformers library to load large language models. Next, we will generate text from these models. Finally, we will finetune models on two tasks (sentiment analysis and machine translation).\n\nThis project will be more open ended than the previous projects. We expect you to learn how to use the huggingface and torch documentation.","metadata":{"id":"35m1Uv47jiNX"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"acPh_4GwYID0"}},{"cell_type":"markdown","source":"First we install and import the required dependencies. These include:\n* `torch` for modeling and training\n* `transformers` for pre-trained models\n* `datasets` from huggingface to load existing datasets.","metadata":{"id":"c7cggO7mjZ5L"}},{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install datasets\n!pip install --upgrade sacrebleu sentencepiece\n\n# Standard library imports\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelWithLMHead","metadata":{"id":"4LV8KY6_unfe","execution":{"iopub.status.busy":"2023-04-08T04:36:23.046497Z","iopub.execute_input":"2023-04-08T04:36:23.046789Z","iopub.status.idle":"2023-04-08T04:37:06.508420Z","shell.execute_reply.started":"2023-04-08T04:36:23.046759Z","shell.execute_reply":"2023-04-08T04:37:06.507280Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\nWe'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging.","metadata":{"id":"KNT_TURTwIlW"}},{"cell_type":"code","source":"assert torch.cuda.is_available()\ndevice = torch.device(\"cuda\")\nprint(\"Using device:\", device)","metadata":{"id":"KYpIPtqtwVwh","execution":{"iopub.status.busy":"2023-04-08T04:37:06.511662Z","iopub.execute_input":"2023-04-08T04:37:06.512974Z","iopub.status.idle":"2023-04-08T04:37:06.585078Z","shell.execute_reply.started":"2023-04-08T04:37:06.512933Z","shell.execute_reply":"2023-04-08T04:37:06.583812Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading Model","metadata":{"id":"ANK-5cMtYSyH"}},{"cell_type":"markdown","source":"We will use GPT-2 medium for this project. This includes both the GPT-2 tokenizer and the GPT-2 model weights itself. If you want to learn more about this model, you can read the GPT-2 paper https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n\nLet's first load the tokenizer for the GPT-2 medium model. You can find how to do this by reading the documentation for AutoTokenzier in transformers, and finding the GPT-2 model of ~345 million params in there.","metadata":{"id":"We4sTUA5j0Ab"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Your code here\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")","metadata":{"id":"Gr9PbArCvCT0","execution":{"iopub.status.busy":"2023-04-08T04:38:58.934205Z","iopub.execute_input":"2023-04-08T04:38:58.934725Z","iopub.status.idle":"2023-04-08T04:39:01.915171Z","shell.execute_reply.started":"2023-04-08T04:38:58.934683Z","shell.execute_reply":"2023-04-08T04:39:01.914106Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"870f1aaaffd941ab95506c0393966ceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c41342e807453a941be9f80025f6b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28d02dd8559488a9fa4ef6662dc1918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d7461a1abe47b196950ca1eefa23ad"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's tokenize and detokenize some text from this model.","metadata":{"id":"V0rkjeg5bezU"}},{"cell_type":"code","source":"print(tokenizer.encode('Hello world'))\nprint(tokenizer.decode(tokenizer.encode('Hello world')))\nprint(tokenizer.encode(\"Hola, cÃ³mo estÃ¡sðŸ˜\"))","metadata":{"id":"HALePz7Cbjzj","execution":{"iopub.status.busy":"2023-04-08T04:39:09.130824Z","iopub.execute_input":"2023-04-08T04:39:09.131328Z","iopub.status.idle":"2023-04-08T04:39:09.146603Z","shell.execute_reply.started":"2023-04-08T04:39:09.131289Z","shell.execute_reply":"2023-04-08T04:39:09.145481Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[15496, 995]\nHello world\n[39, 5708, 11, 269, 10205, 5908, 1556, 40138, 47249, 235]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's load the GPT-2 medium model. Make sure you also put the model onto the GPU.","metadata":{"id":"v4aSFr-hmKpw"}},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead\n# Your code here\ngpt2_model = AutoModelWithLMHead.from_pretrained(\"gpt2-medium\").to('cuda')","metadata":{"id":"W1owrfI1xjby","execution":{"iopub.status.busy":"2023-04-08T05:54:26.575998Z","iopub.execute_input":"2023-04-08T05:54:26.576738Z","iopub.status.idle":"2023-04-08T05:54:39.726804Z","shell.execute_reply.started":"2023-04-08T05:54:26.576699Z","shell.execute_reply":"2023-04-08T05:54:39.725734Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:1299: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generate From the Model","metadata":{"id":"k1aBbR3snOjV"}},{"cell_type":"markdown","source":"Now let's generate some text from the model to test its LM capabilities. Let's generate 10 pieces of random text of length 50 tokens from the model using random sampling with temperature set to 0.7. This will allow the text to be somewhat high in diversity (random sampling) while maintaining reasonable quality (temperature < 1). When generating text, you can condition on phrases such as \"The coolest thing in NLP right now is\". Find the relevant function and arguments to use for generating text using the Huggingface documentation.\n\nHint: you may find https://huggingface.co/docs/transformers/main_classes/text_generation to be useful for learning about generating from LMs.","metadata":{"id":"5Mo0tnIbnQz8"}},{"cell_type":"code","source":"inputs = tokenizer(\"<|startoftext|>The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\nsample_outputs = gpt2_model.generate(\n    inputs, num_return_sequences=10, do_sample=True, temperature=0.7, max_length=50, top_k=0, \n)","metadata":{"id":"8xSUaso9vo1V","execution":{"iopub.status.busy":"2023-04-08T04:41:55.495944Z","iopub.execute_input":"2023-04-08T04:41:55.496917Z","iopub.status.idle":"2023-04-08T04:41:59.202562Z","shell.execute_reply.started":"2023-04-08T04:41:55.496879Z","shell.execute_reply":"2023-04-08T04:41:59.201490Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Now lets print the text.","metadata":{"id":"7uVROVwndt_z"}},{"cell_type":"code","source":"for i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"id":"koCYZxALdvjX","execution":{"iopub.status.busy":"2023-04-08T04:42:21.675489Z","iopub.execute_input":"2023-04-08T04:42:21.675877Z","iopub.status.idle":"2023-04-08T04:42:21.684315Z","shell.execute_reply.started":"2023-04-08T04:42:21.675844Z","shell.execute_reply":"2023-04-08T04:42:21.683162Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"0: <|startoftext|>The coolest thing right now in NLP is machine learning.\n\n|endoftext|>Networking is ever changing.\n\n|endoftext|>Postgres is quite the contrary.\n\n\n1: <|startoftext|>The coolest thing right now in NLP is the ability to identify a person's face in-game. It's not a gimmick, but it's cool. So when you're trying to identify a face from a\n2: <|startoftext|>The coolest thing right now in NLP is Learning by Example, a tool that lets you follow a user's text as they type into a browser. You can use it to learn a language or just observe their language\n3: <|startoftext|>The coolest thing right now in NLP is more efficient object detection based on machine learning. Although that is not an official EDA (even though I am using it), I am able to detect recursively changes\n4: <|startoftext|>The coolest thing right now in NLP is :epub|<|print|>The beloved, the beloved one. :epub|<|print|>The one that's always shifting the needle and\n5: <|startoftext|>The coolest thing right now in NLP is Machine Learning.\n\njSkillsLucid - Jul 29, 2018\n\ni really enjoy the program.\n6: <|startoftext|>The coolest thing right now in NLP is machine translation. Machine translation is the process of extracting the meanings of human language words from another language and comparing them to parse trees, where the meaning is translated from one language\n7: <|startoftext|>The coolest thing right now in NLP is that it's now possible to extract a single word from a sentence. Most of the time you can tell what a word means, but sometimes you have to guess what it\n8: <|startoftext|>The coolest thing right now in NLP is the fact that you can use Python to query a database through a RESTful API or you could use a domain specific language like Ruby or Python.\n\nEric: So\n9: <|startoftext|>The coolest thing right now in NLP is deriving representations from the data. It's awesome!\n\nMales.and.Males\n\nThe world's first voting system for voting.\n\nM\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now generate one piece of text of length 50 with the same prompt (\"The coolest thing right now in NLP is\") but use greedy decoding (temperature = 0). This roughly corresponds to generating some text that is high likelihood for the model.","metadata":{"id":"ox_6NYWCoydJ"}},{"cell_type":"code","source":"inputs = tokenizer(\"<|startoftext|>The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\nsample_outputs = gpt2_model.generate(\n    inputs, do_sample=False, temperature=0, max_length=50\n)","metadata":{"id":"EQEwpPUHePmC","execution":{"iopub.status.busy":"2023-04-08T04:44:01.961237Z","iopub.execute_input":"2023-04-08T04:44:01.961832Z","iopub.status.idle":"2023-04-08T04:44:03.435663Z","shell.execute_reply.started":"2023-04-08T04:44:01.961796Z","shell.execute_reply":"2023-04-08T04:44:03.434536Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now let's try to see how good of a translation system GPT-2 medium is when used \"out of the box\". To accomplish this, we can condition on a prompt like the one below and generate from the model with greedy decoding. This will attempt to translate the sentence \"UC Berkeley ist eine Schule in Kalifornien\", which means \"UC Berkeley is a school in California\". Make sure to set the max length to be high enough so that the model generates sufficient text.","metadata":{"id":"fjVBlj9yfPbx"}},{"cell_type":"code","source":"prompt = \"\"\"Translate the following texts into English.\n\nGerman: UC Berkeley ist eine Schule in Kalifornien\nEnglish:\"\"\"","metadata":{"id":"Xj-OHYlppX4N","execution":{"iopub.status.busy":"2023-04-08T04:45:43.236051Z","iopub.execute_input":"2023-04-08T04:45:43.237018Z","iopub.status.idle":"2023-04-08T04:45:43.241101Z","shell.execute_reply.started":"2023-04-08T04:45:43.236980Z","shell.execute_reply":"2023-04-08T04:45:43.240219Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Your code here. Generate from the model using greedy decoding with the above prompt\neos_token_id = tokenizer.eos_token_id\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\ngreedy_output = gpt2_model.generate(\n    inputs, do_sample=False, max_length=50, temperature=0, eos_token_id=eos_token_id,\n)\nprint(tokenizer.decode(greedy_output[0]))","metadata":{"id":"VfqUVa8NfO2j","execution":{"iopub.status.busy":"2023-04-08T04:46:01.193881Z","iopub.execute_input":"2023-04-08T04:46:01.194275Z","iopub.status.idle":"2023-04-08T04:46:01.611757Z","shell.execute_reply.started":"2023-04-08T04:46:01.194240Z","shell.execute_reply":"2023-04-08T04:46:01.610746Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\nGerman: UC Berkeley ist eine Schule in Kalifornien\nEnglish: UC Berkeley ist eine Schule in Kalifornien\n\nEnglish: UC Berkeley ist e\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see, translation quality is terrible, as it just repeats the words from the previous text.","metadata":{"id":"TBGuw-18qjsB"}},{"cell_type":"markdown","source":"Now, let's finetune GPT-2 on the translation task to improve the results. We will use a translation dataset from the Huggingface dataset repository (it has thousands of other datasets available). This dataset is one of TED talks translated between German and English.","metadata":{"id":"GlBIoN4aragA"}},{"cell_type":"code","source":"import datasets\ndataset = datasets.load_dataset(\"ted_talks_iwslt\", language_pair=(\"de\", \"en\"), year=\"2014\")","metadata":{"id":"SJYzMQxfvrr0","execution":{"iopub.status.busy":"2023-04-08T04:46:45.652739Z","iopub.execute_input":"2023-04-08T04:46:45.653778Z","iopub.status.idle":"2023-04-08T04:47:43.319239Z","shell.execute_reply.started":"2023-04-08T04:46:45.653732Z","shell.execute_reply":"2023-04-08T04:47:43.317785Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21cfac6346cd4527bee20a6ca717d47c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3bcc19c7ba04520b6984491987f88d2"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset ted_talks_iwslt/de_en_2014 to /root/.cache/huggingface/datasets/ted_talks_iwslt/de_en_2014-c6e771351acd148b/1.1.0/43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"240d5aebd6bb48eea3944bb26841835b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset ted_talks_iwslt downloaded and prepared to /root/.cache/huggingface/datasets/ted_talks_iwslt/de_en_2014-c6e771351acd148b/1.1.0/43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843e3d10d5c641069eff652b175e72d5"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"Xm-2v9h3mav8","execution":{"iopub.status.busy":"2023-04-08T04:47:43.322337Z","iopub.execute_input":"2023-04-08T04:47:43.323190Z","iopub.status.idle":"2023-04-08T04:47:43.328639Z","shell.execute_reply.started":"2023-04-08T04:47:43.323153Z","shell.execute_reply":"2023-04-08T04:47:43.327158Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(dataset['train'][0]['translation'])","metadata":{"id":"OSvnbe8Bj8f0","execution":{"iopub.status.busy":"2023-04-08T04:47:43.333269Z","iopub.execute_input":"2023-04-08T04:47:43.333914Z","iopub.status.idle":"2023-04-08T04:47:43.344863Z","shell.execute_reply.started":"2023-04-08T04:47:43.333880Z","shell.execute_reply":"2023-04-08T04:47:43.343963Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"{'de': '\"Ich habe Zerebralparese. Ich zappele die ganze Zeit\", kÃ¼ndigt Maysoon Zayid zu Anfang dieses ungeheuer witzigen, erheiternden an. (Er ist wirklich ungeheur witzig.) \"Als wÃ¼rde Shakira auf Muhammad Ali treffen.\" Elegant und scharfsinnig nimmt uns die arabisch-amerikanische Komikerin auf eine Reise durch ihre Abenteuer als Schauspielerin, Komikerin, Philanthropin und FÃ¼rsprecherin fÃ¼r Menschen mit Behinderungen mit.', 'en': '\"I have cerebral palsy. I shake all the time,\" Maysoon Zayid announces at the beginning of this exhilarating, hilarious talk. (Really, it\\'s hilarious.) \"I\\'m like Shakira meets Muhammad Ali.\" With grace and wit, the Arab-American comedian takes us on a whistle-stop tour of her adventures as an actress, stand-up comic, philanthropist and advocate for the disabled.'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can create a dataset. For each element in the dataset, it should have a text prompt and then the translation, similar to above. Your job is to fill in the labels field below. This field sets the labels to use for training during the language modeling task. \n\nFor the labels, we only want to train the model to output the text after the words \"English:\". This is because in the prompt, everything before the words \"English:\" will also be provided to the model as input. Hint: use -100 as the label for tokens you do not want to train on.\nHint 2: When doing LM training, the labels are the same as the input tokens, except shifted to the left by one. You should check whether Huggingface is already doing the shifting, or whether you need to do the shifting yourself.\n\nOne thing to be careful of with all LMs is to make sure there are not extra spaces. So, the text should be formatted as like \"English: Hello...\" not \"English:  Hello...\". This issue is a common problem people face when using APIs like GPT-3 which we will cover next time.","metadata":{"id":"ISt5r0qKnGCg"}},{"cell_type":"code","source":"prompt = \"\"\"Translate the following texts into English.\nGerman: \"\"\"\n\nclass TranslationDataset(Dataset):\n    def __init__(self, examples, tokenizer):\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n        for example in examples:\n            training_text = prompt + example['translation']['de'] + '\\nEnglish: ' + example['translation']['en'] + \"<|endoftext|>\"\n            encodings_dict = tokenizer(training_text, max_length=275, padding=\"max_length\", truncation=True)\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n            prompt_and_input_length = len(tokenizer.encode(prompt + example['translation']['de'] + '\\nEnglish:'))\n            # your code below\n            label = encodings_dict['input_ids']\n            label[:prompt_and_input_length] = [-100] * prompt_and_input_length\n            self.labels.append(torch.tensor(label))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {'input_ids':self.input_ids[idx], 'attention_mask':self.attn_masks[idx], 'labels':self.labels[idx]}","metadata":{"id":"ZtBrXm2Ym4uy","execution":{"iopub.status.busy":"2023-04-08T04:51:21.753350Z","iopub.execute_input":"2023-04-08T04:51:21.754044Z","iopub.status.idle":"2023-04-08T04:51:21.762916Z","shell.execute_reply.started":"2023-04-08T04:51:21.754007Z","shell.execute_reply":"2023-04-08T04:51:21.761744Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"translation_dataset = TranslationDataset(dataset['train'], tokenizer)","metadata":{"id":"rjfb7hkmrAze","execution":{"iopub.status.busy":"2023-04-08T04:51:23.057767Z","iopub.execute_input":"2023-04-08T04:51:23.058350Z","iopub.status.idle":"2023-04-08T04:51:25.662175Z","shell.execute_reply.started":"2023-04-08T04:51:23.058312Z","shell.execute_reply":"2023-04-08T04:51:25.661134Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Now let's break the dataset into a train and test split.","metadata":{"id":"l0xe93WarEOd"}},{"cell_type":"code","source":"train_size = int(0.9 * len(translation_dataset))\ntrain_dataset, val_dataset = random_split(translation_dataset, [train_size, len(translation_dataset) - train_size])\nprint(len(train_dataset))\nprint(len(val_dataset))","metadata":{"id":"1cV_oz5rpdOe","execution":{"iopub.status.busy":"2023-04-08T04:51:25.664224Z","iopub.execute_input":"2023-04-08T04:51:25.664684Z","iopub.status.idle":"2023-04-08T04:51:25.672458Z","shell.execute_reply.started":"2023-04-08T04:51:25.664645Z","shell.execute_reply":"2023-04-08T04:51:25.671312Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"2674\n298\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_dataset[0])","metadata":{"id":"Ijr5Pn_uqk5e","execution":{"iopub.status.busy":"2023-04-08T04:51:27.137361Z","iopub.execute_input":"2023-04-08T04:51:27.138290Z","iopub.status.idle":"2023-04-08T04:51:27.149831Z","shell.execute_reply.started":"2023-04-08T04:51:27.138232Z","shell.execute_reply":"2023-04-08T04:51:27.148770Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([ 8291, 17660,   262,  1708, 13399,   656,  3594,    13,   198, 16010,\n           25, 14236,   666, 15617,   647,    83,  3222, 33565,   861,   304,\n          500,   285,  9101,    70,   677,   258,  1168,  2724,   403,   701,\n          748,  7157,   893,   257,  3046, 38436,    87, 24814,  2815,   532,\n          304,   259,  7157,    88,    11,   288,   292,   288,  2575,  5178,\n           86,   392,    75,  2150,  3318,   402,   413,   488,   912,   332,\n           75,  3536,  2150,  6188,   268,  1729,  4703,   518,   297,   288,\n          283,   301,   695,    83,  3318,   523,   304,   500,   302,   528,\n           85,   692, 19933,  3683,  4587,   509,  2002,   403,  1134,   341,\n         1931,    76,  9101,  4743, 30830,    13,   198, 15823,    25, 14236,\n          666, 15617,   647,    83, 35551,   530,  2003,   286,   262,  5175,\n         3072,  1377,   257,  5485,    12,  1477, 13309,   290,  3463,    12,\n         1477, 13309, 40445,   326,   366,  6381, 26024,     1,  1321,  1729,\n         4703,   935,    11,  6011,   257, 10974,  2759, 19933,   835,   284,\n        10996,    13,  1279,   368, 33994, 11928,  1150,   379,  1279,    64,\n        13291,    28,  4023,  1378,  2503,    13,  1513,    13,   785,    14,\n         1513,    87,    14, 31534,    14,  1485,  2598,    29, 36493,    87,\n        24814,  2815,  3556,    64,    29,  2014,  3556,   368,    29, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 14236,\n          666, 15617,   647,    83, 35551,   530,  2003,   286,   262,  5175,\n         3072,  1377,   257,  5485,    12,  1477, 13309,   290,  3463,    12,\n         1477, 13309, 40445,   326,   366,  6381, 26024,     1,  1321,  1729,\n         4703,   935,    11,  6011,   257, 10974,  2759, 19933,   835,   284,\n        10996,    13,  1279,   368, 33994, 11928,  1150,   379,  1279,    64,\n        13291,    28,  4023,  1378,  2503,    13,  1513,    13,   785,    14,\n         1513,    87,    14, 31534,    14,  1485,  2598,    29, 36493,    87,\n        24814,  2815,  3556,    64,    29,  2014,  3556,   368,    29, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can use the Huggingface Trainer to finetune GPT-2 on this dataset. This abstracts away all of the details of training. Setup the training arguments to perform 3 epochs of training on this dataset, use a per-device batch size of 2 with gradient accumulation set to 8, use 100 warmup steps, a weight decay of 0.05. Set the eval batch size to be 2. Save a checkpoint every 250 steps. Set fp16 to True. Save the checkpoint in a specific output_dir so you can load it later. Hint: if it tries to launch Wandb, you may add the argument report_to=\"none\".","metadata":{"id":"1sLfI6vDriWK"}},{"cell_type":"code","source":"# Your code here\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2_translation_checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=100,\n    weight_decay=0.05,\n    save_steps=250,\n    fp16=True,\n    report_to=\"none\" \n)","metadata":{"id":"eC4S6WBCsjOW","execution":{"iopub.status.busy":"2023-04-08T04:53:45.062480Z","iopub.execute_input":"2023-04-08T04:53:45.063423Z","iopub.status.idle":"2023-04-08T04:53:45.070937Z","shell.execute_reply.started":"2023-04-08T04:53:45.063369Z","shell.execute_reply":"2023-04-08T04:53:45.069931Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Next create a Huggingface Trainer object and call train() on it.","metadata":{"id":"qsxRb6TOVHn5"}},{"cell_type":"code","source":"# Your code here\ntrainer = Trainer(\n    model=gpt2_model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n)\ntrainer.train()","metadata":{"id":"xheKi30BVVJC","execution":{"iopub.status.busy":"2023-04-08T04:53:55.636385Z","iopub.execute_input":"2023-04-08T04:53:55.637335Z","iopub.status.idle":"2023-04-08T05:13:03.813141Z","shell.execute_reply.started":"2023-04-08T04:53:55.637282Z","shell.execute_reply":"2023-04-08T05:13:03.812071Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [501/501 19:05, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.859900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=501, training_loss=0.8586751594990789, metrics={'train_runtime': 1148.129, 'train_samples_per_second': 6.987, 'train_steps_per_second': 0.436, 'total_flos': 3998491818393600.0, 'train_loss': 0.8586751594990789, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Now load your saved checkpoint and see how well the finetuned GPT-2 model does on translating the sentence from before.","metadata":{"id":"SnbquLGFx0Y2"}},{"cell_type":"code","source":"prompt = \"\"\"Translate the following texts into English.\n\nGerman: UC Berkeley ist eine Schule in Kalifornien\nEnglish:\"\"\"","metadata":{"id":"aOIL0Ha_Fqk3","execution":{"iopub.status.busy":"2023-04-08T05:15:46.716864Z","iopub.execute_input":"2023-04-08T05:15:46.717440Z","iopub.status.idle":"2023-04-08T05:15:46.722992Z","shell.execute_reply.started":"2023-04-08T05:15:46.717398Z","shell.execute_reply":"2023-04-08T05:15:46.721924Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Your code here\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\ngreedy_output = gpt2_model.generate(\n    inputs, do_sample=False, max_length=50, temperature=0, eos_token_id=eos_token_id,\n)\nprint(tokenizer.decode(greedy_output[0]))","metadata":{"id":"3IR7l2P_L9ey","execution":{"iopub.status.busy":"2023-04-08T05:58:05.089922Z","iopub.execute_input":"2023-04-08T05:58:05.090948Z","iopub.status.idle":"2023-04-08T05:58:05.128156Z","shell.execute_reply.started":"2023-04-08T05:58:05.090908Z","shell.execute_reply":"2023-04-08T05:58:05.127143Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\n        German: Ein MÃ¤dchen an einer KÃ¼ste mit einem Berg im Hintergrund.\n        English: The\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If training went correctly, you should see a reasonable translation of the sentence, with some errors.\n\nFor the project report, find two sentences where the model succeeds and two sentences where the model fails. Describe what might be causing these types of failures.","metadata":{"id":"EUCG_xSYN2pn"}},{"cell_type":"code","source":"# Find two sentences where the model succeeds and two sentences where the model fails\nprompt = \"\"\"Translate the following texts into English.\n\nGerman: Der Mann ist im Wasser.\nEnglish:\"\"\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\ngreedy_output = gpt2_model.generate(\n    inputs, do_sample=False, max_length=50, temperature=0, eos_token_id=eos_token_id,\n)\nprint(tokenizer.decode(greedy_output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-08T05:44:32.713386Z","iopub.execute_input":"2023-04-08T05:44:32.714116Z","iopub.status.idle":"2023-04-08T05:44:32.869349Z","shell.execute_reply.started":"2023-04-08T05:44:32.714059Z","shell.execute_reply":"2023-04-08T05:44:32.868123Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\nGerman: Der Mann ist im Wasser.\nEnglish: The man is the ocean.<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Finally, revisit the code from project 2 on using and running the Multi30k dataset. Your goal will be to translate the test set using the GPT-2 model you just finetuned. You will then submit your test predictions as a txt file, where you place your model's prediction for each test example on a separate line. Feel free to copy and paste any code from HW2 that may be useful. Submit the file named as mt_predictions.txt to gradescope.\n\nThe GPT-2 model may not work that well on the Multi30k dataset, because there is a distribution shift where the Multi30k data looks different than the Ted talks data that you finetuned the model on. The takeaway I want people to have is that a general-purpose LM system can be decent at a task like translation, however, if you create a domain-specific model like a LSTM trained specifically on Multi30k, you can outperform the general purpose model.\n\nFor the project report, compare two translations from the GPT-2 versus LSTM model. Which one works better?\n\nHint: One failure mode for GPT-2 is that it may generate fluent sentences that are actually unrelated to the input.","metadata":{"id":"qC_sFmejXEtA"}},{"cell_type":"code","source":"# Your code for generating mt_predictions.txt below\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nimport sacrebleu\nmydataset = load_dataset('bentrevett/multi30k')\ntest_dataset = mydataset['test']\ntarget_sentences = []\npredictions = []\nwith open(\"mt_predictions.txt\", \"w\") as f:\n    for sentence_pair in tqdm(test_dataset):\n        prompt = \"\"\"Translate the following texts into English.\n\n        German: {}\n        English:\"\"\".format(sentence_pair['de'])\n        target_sentences.append(sentence_pair['en'])\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n        input_len = len(input_ids[0])\n        greedy_output = gpt2_model.generate(\n            input_ids, do_sample=False, max_length=100, temperature=0, eos_token_id=eos_token_id\n        )\n        prediction = tokenizer.decode(greedy_output[0][input_len:-1], skip_special_tokens=True)\n        predictions.append(prediction)\n        f.write(prediction+'\\n')\nprint(sacrebleu.corpus_bleu(predictions, [target_sentences]).score)","metadata":{"id":"hRws9idEXecL","execution":{"iopub.status.busy":"2023-04-08T05:48:35.809163Z","iopub.execute_input":"2023-04-08T05:48:35.810199Z","iopub.status.idle":"2023-04-08T05:53:52.477229Z","shell.execute_reply.started":"2023-04-08T05:48:35.810155Z","shell.execute_reply":"2023-04-08T05:53:52.475996Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/bentrevett--multi30k to /root/.cache/huggingface/datasets/json/bentrevett--multi30k-fd2305abd2b24ec2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65c182e1e77548d58611637b52c55627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.60M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43f01959dbd1464ca48bb483a5fced25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/156k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d56ac0bc3d44eed8ea030c6c0a8d45a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1efaa96601724561b7723bec5900ef99"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/bentrevett--multi30k-fd2305abd2b24ec2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b0e9f4f384040f69196be98e91120aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c618c5f0874050b3977347e4ec756a"}},"metadata":{}},{"name":"stdout","text":"6.988990202838409\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Sentiment Analysis","metadata":{"id":"y0WsHbgnOAFx"}},{"cell_type":"markdown","source":"The beauty of language models is that we can apply this exact same machinery to solve a completely different task of sentiment analysis. Here, we will be given a movie review and the goal is to have the model predict whether the review is positive or negative.","metadata":{"id":"3VmRQfoyOA2G"}},{"cell_type":"markdown","source":"First, we will load some sentiment analysis data. Your job is to copy what we did above for machine translation to load the dataset, build a Class to create the dataset, etc., \n\nWhen doing so, use the prompt below, where you put the text of the input in the first [] and in the second [], put the word Positive if the label is 1 and the word Negative if the label is 0. Make sure to also set the self.labels field correctly, we only want to compute a loss on the words Positive/Negative, and no other tokens in the model's input.\n\nThe following is a movie review. [Movie Review Text Here]. The sentiment of the review is [Positive/Negative].","metadata":{"id":"JhJ5ptLyOoGI"}},{"cell_type":"code","source":"import datasets\ndataset = load_dataset('glue', 'sst2')","metadata":{"id":"Qk67kbfPQAGy","execution":{"iopub.status.busy":"2023-04-08T06:20:46.880315Z","iopub.execute_input":"2023-04-08T06:20:46.880708Z","iopub.status.idle":"2023-04-08T06:20:47.612728Z","shell.execute_reply.started":"2023-04-08T06:20:46.880674Z","shell.execute_reply":"2023-04-08T06:20:47.611629Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59eb28fecc3f43228ec6a568ad1bddd2"}},"metadata":{}}]},{"cell_type":"markdown","source":"Note: Some people were saying that this line of code wasn't working and they needed to use \"dataset = datasets.load_dataset('glue', 'sst2')\" instead.","metadata":{"id":"tJ0CbUm8L0ZR"}},{"cell_type":"code","source":"class SentimentDataset(Dataset):\n    # Your code below\n    def __init__(self, examples, tokenizer):\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n        self.label = ['Negative', 'Positive']\n        for example in tqdm(examples):\n            training_text = prompt + example['sentence'] + 'The sentiment of the review is ' + self.label[example['label']] + ' <|endoftext|>'\n            encodings_dict = tokenizer(training_text, max_length=275, padding=\"max_length\", truncation=True)\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n            prompt_and_input_length = len(tokenizer(prompt + example['sentence'] + 'The sentiment of the review is ')['input_ids'])\n            label = encodings_dict['input_ids']\n            label[:prompt_and_input_length-1] = [-100]*(prompt_and_input_length-1)\n            self.labels.append(torch.tensor(label))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.input_ids[idx], \n            'attention_mask': self.attn_masks[idx], \n            'labels': self.labels[idx]\n        }","metadata":{"id":"j7_GzIgmRIGC","execution":{"iopub.status.busy":"2023-04-08T06:20:47.614771Z","iopub.execute_input":"2023-04-08T06:20:47.615853Z","iopub.status.idle":"2023-04-08T06:20:47.628036Z","shell.execute_reply.started":"2023-04-08T06:20:47.615814Z","shell.execute_reply":"2023-04-08T06:20:47.626854Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"sentiment_train_dataset = SentimentDataset(dataset['train'], tokenizer)\nsentiment_val_dataset = SentimentDataset(dataset['validation'], tokenizer)","metadata":{"id":"2lsk5uGNRIGE","execution":{"iopub.status.busy":"2023-04-08T06:20:47.631142Z","iopub.execute_input":"2023-04-08T06:20:47.631543Z","iopub.status.idle":"2023-04-08T06:21:52.577114Z","shell.execute_reply.started":"2023-04-08T06:20:47.631515Z","shell.execute_reply":"2023-04-08T06:21:52.575980Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/67349 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a276dfebe54af8acc9a3178684751d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/872 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30d510c206948e48366eaa69ad325eb"}},"metadata":{}}]},{"cell_type":"markdown","source":"The data already comes with a validation and train split","metadata":{"id":"SUye-UIRRIGF"}},{"cell_type":"code","source":"print(len(sentiment_train_dataset))\nprint(len(sentiment_val_dataset))","metadata":{"id":"Kb0XeLriRIGF","execution":{"iopub.status.busy":"2023-04-08T06:21:52.579910Z","iopub.execute_input":"2023-04-08T06:21:52.580626Z","iopub.status.idle":"2023-04-08T06:21:52.587286Z","shell.execute_reply.started":"2023-04-08T06:21:52.580586Z","shell.execute_reply":"2023-04-08T06:21:52.586187Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"67349\n872\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's train the model using the same trainer arguments as before, except just do $<$1 epoch of training because this dataset is quite large and training on the entire thing will take some time. Make sure you also use a different output_dir so it doesn't overwrite your old results.","metadata":{"id":"Q7vOS-ZcTHds"}},{"cell_type":"code","source":"# Your code here\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2_sentiment_checkpoints\",\n    num_train_epochs=0.05,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=100,\n    weight_decay=0.05,\n    save_steps=250,\n    fp16=True,\n    report_to=\"none\", \n)\n\nsentiment_gpt2_model = AutoModelWithLMHead.from_pretrained(\"gpt2-medium\").to('cuda')\ntrainer = Trainer(\n    model=sentiment_gpt2_model, args=training_args, train_dataset=sentiment_train_dataset, eval_dataset=sentiment_val_dataset\n)\ntrainer.train()","metadata":{"id":"vEAs8zFDVdY4","execution":{"iopub.status.busy":"2023-04-08T06:21:52.589044Z","iopub.execute_input":"2023-04-08T06:21:52.589835Z","iopub.status.idle":"2023-04-08T06:29:52.035794Z","shell.execute_reply.started":"2023-04-08T06:21:52.589799Z","shell.execute_reply":"2023-04-08T06:29:52.034859Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:1299: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [211/211 07:51, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=211, training_loss=1.1155539417718825, metrics={'train_runtime': 473.1975, 'train_samples_per_second': 7.116, 'train_steps_per_second': 0.446, 'total_flos': 1683995556249600.0, 'train_loss': 1.1155539417718825, 'epoch': 0.05})"},"metadata":{}}]},{"cell_type":"markdown","source":"At test-time, when you want to classify an incoming movie review, you can just check whether the model generates the words Positive or Negative as the final word.","metadata":{"id":"azaXJt4cPV3Y"}},{"cell_type":"code","source":"prompt = \"\"\"The following is a movie review. The acting was great but overall I was left disappointed by the film. The sentiment of the review is\"\"\"","metadata":{"id":"wlTivogUUyFz","execution":{"iopub.status.busy":"2023-04-08T06:30:09.476991Z","iopub.execute_input":"2023-04-08T06:30:09.477727Z","iopub.status.idle":"2023-04-08T06:30:09.482233Z","shell.execute_reply.started":"2023-04-08T06:30:09.477689Z","shell.execute_reply":"2023-04-08T06:30:09.481165Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Your code here\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\ngreedy_output = sentiment_gpt2_model.generate(\n    inputs, do_sample=False, max_length=500, temperature=0, eos_token_id=eos_token_id,\n)\nprint(tokenizer.decode(greedy_output[0]))","metadata":{"id":"l3-2HzbHVJ9L","execution":{"iopub.status.busy":"2023-04-08T06:30:09.755850Z","iopub.execute_input":"2023-04-08T06:30:09.756974Z","iopub.status.idle":"2023-04-08T06:30:09.831188Z","shell.execute_reply.started":"2023-04-08T06:30:09.756926Z","shell.execute_reply":"2023-04-08T06:30:09.830056Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"The following is a movie review. The acting was great but overall I was left disappointed by the film. The sentiment of the review is Negative <|endoftext|>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Finally, run the entire validation set through the model and get your model predictions. Save the results as a txt file, where each line just contains either \"1\" if your model predicted Positive and \"0\" if the model predicted Negative. You will get full credit if your model's accuracy is greater than 80%. Save the file as sst_predictions.txt and submit it to gradescope.\n\nFor the report, describe two possible improvements to your sentiment classifier.","metadata":{"id":"nc5bgcS7X1BX"}},{"cell_type":"code","source":"# Your code here for generating sst_predictions\nfrom sklearn.metrics import accuracy_score\ndef label_check(l):\n    if l==33733:\n        return 1\n    elif l==36183:\n        return 0\n    else:\n        return 0\n    \ntest_dataset = sentiment_val_dataset\ntargets = []\npredictions = []\nwith open(\"sst_predictions.txt\", \"w\") as f:\n    for text in tqdm(test_dataset):\n        inputs = text['input_ids'].clone()\n        label_pos = torch.where(inputs == 220)[0].tolist()[0]-1\n        label = inputs[label_pos].item()\n        targets.append(label_check(label))\n        inputs = inputs[:label_pos]\n        inputs = inputs.unsqueeze(0).to('cuda')\n        greedy_output = sentiment_gpt2_model.generate(\n            inputs, do_sample=False, max_length=500, temperature=0\n        )\n        prediction = label_check(greedy_output[0][label_pos].item())\n        predictions.append(prediction)\n        f.write(str(prediction)+'\\n')\n                          \nacc = accuracy_score(targets, predictions)\nprint(acc)","metadata":{"id":"wnmzEl7wYGW2","execution":{"iopub.status.busy":"2023-04-08T06:30:11.481267Z","iopub.execute_input":"2023-04-08T06:30:11.481981Z","iopub.status.idle":"2023-04-08T06:31:09.597242Z","shell.execute_reply.started":"2023-04-08T06:30:11.481941Z","shell.execute_reply":"2023-04-08T06:31:09.596164Z"},"trusted":true},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/872 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6793ab83b24bc1a0ede4424e1936dd"}},"metadata":{}},{"name":"stdout","text":"0.9059633027522935\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Submission","metadata":{"id":"kotdLszxHWWJ"}},{"cell_type":"markdown","source":"Turn in the following files on Gradescope:\n* hw4.ipynb (this file; please rename to match)\n* mt_predictions.txt (the predictions for the Multi30k test set)\n* sst_predictions.txt (the predictions for the SST-2 validation set)\n* report.pdf\n\nBe sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.","metadata":{"id":"-onu93vgG2-U"}}]}